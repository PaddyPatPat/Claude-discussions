#+TITLE: Student Argument Analysis with NLP
#+AUTHOR: Claude
#+DATE: [2025-11-08]
#+TAGS: nlp argument-mining discourse-analysis education

* Overview

Methodology for using NLP to analyze student argumentative essays, identify agreement/disagreement patterns, and extract insights about student reasoning across a class.

* Use Case Definition

** Problem Statement

Analyze a collection of student papers arguing for/against various topics to:
- Identify where students agree and disagree
- Extract argument patterns and structures
- Find consensus and dissensus on specific topics
- Map ideological clustering among students
- Understand reasoning strategies and evidence use

** Input Data

- Collection of student papers in markdown format
- Papers contain arguments for/against specific topics
- Variable length and structure
- Need to track changes across multiple analysis iterations

** Desired Outputs

- Agreement/disagreement matrices
- Argument clusters and patterns
- Consensus topic identification
- Unique perspective detection
- Evidence type classification
- Student similarity networks

* Argument Mining and Discourse Analysis

** What is Argument Mining?

Argument mining is the automatic identification and extraction of argumentative structures from text, including:

- **Claims**: Main assertions or positions
- **Evidence**: Supporting facts, data, or reasoning
- **Warrants**: Logical connections between claims and evidence
- **Counterarguments**: Acknowledgment of opposing views
- **Rebuttals**: Responses to counterarguments

** Comparative Discourse Analysis

Analyzing how different authors (students) discuss the same topics:
- Stance detection (for/against specific positions)
- Framing analysis (how issues are presented)
- Evidence comparison (what sources/types used)
- Rhetorical strategy identification

* Iterative NLP Development Workflow

** Why Iteration is Essential

In exploratory NLP analysis, you don't know what you're looking for until you start looking. Each pass through the data reveals:
- New patterns and themes
- Better ways to operationalize concepts
- Refined understanding of the corpus
- Additional questions to investigate

** Typical Workflow Phases

*** Phase 1: Initial Exploration

#+BEGIN_SRC python
# Basic preprocessing and exploration
import spacy

nlp = spacy.load("en_core_web_sm")

for paper in student_papers:
    doc = nlp(paper.content)
    # Basic statistics
    print(f"Tokens: {len(doc)}")
    print(f"Sentences: {len(list(doc.sents))}")
    # Word frequencies
    # Simple sentiment
#+END_SRC

**Goals**:
- Understand corpus characteristics
- Identify basic patterns
- Spot obvious themes

*** Phase 2: Concept Discovery

#+BEGIN_SRC python
# Identify key themes and arguments
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Topic modeling to find what students argue about
vectorizer = TfidfVectorizer(max_features=1000)
tfidf = vectorizer.fit_transform(documents)

lda = LatentDirichletAllocation(n_components=10)
topics = lda.fit_transform(tfidf)

# Sentiment toward topics
# Argument structure identification
#+END_SRC

**Goals**:
- Discover main themes
- Identify argument patterns
- Find linguistic patterns

*** Phase 3: Feature Refinement

#+BEGIN_SRC python
# Develop better concept extraction based on discoveries

def extract_refined_arguments(doc):
    """
    Extract arguments using insights from initial exploration.
    """
    claims = []
    evidence = []

    # Use patterns discovered in phase 2
    for sent in doc.sents:
        if matches_claim_pattern(sent):
            claims.append(sent)
        if matches_evidence_pattern(sent):
            evidence.append(sent)

    return {"claims": claims, "evidence": evidence}
#+END_SRC

**Goals**:
- Operationalize discovered concepts
- Create better feature extractors
- Build custom classifiers

*** Phase 4: Re-analysis with Refined Understanding

#+BEGIN_SRC python
# Apply refined understanding back to corpus

iteration_results = []

for paper in student_papers:
    doc = nlp(paper.content)

    # Apply refined argument extraction
    arguments = extract_refined_arguments(doc)

    # Apply discovered patterns
    patterns = identify_argument_patterns(arguments)

    # Store with iteration marker
    iteration_results.append({
        "student_id": paper.id,
        "iteration": 2,
        "arguments": arguments,
        "patterns": patterns,
        "timestamp": datetime.now()
    })
#+END_SRC

**Goals**:
- Re-analyze with better tools
- Validate discoveries
- Find deeper insights

*** Phase 5: Validation and Comparison

#+BEGIN_SRC python
# Compare iterations and validate findings

def compare_iterations(iteration_1, iteration_2):
    """
    Compare findings across iterations to ensure consistency.
    """
    # Check if key patterns remain stable
    # Identify what changed with better tools
    # Validate interpretations
    pass
#+END_SRC

**Goals**:
- Ensure findings are robust
- Test interpretations
- Prepare for reporting

** Iteration Frequency

- **Initial passes**: Daily as you explore
- **Refinement cycles**: Weekly as patterns emerge
- **Validation rounds**: When preparing to report

* Recommended Tool Stack for Student Papers

** Phase 1: Exploratory Analysis

Use **spaCy** or **NLTK**:

#+BEGIN_SRC python
import spacy
from collections import Counter

nlp = spacy.load("en_core_web_sm")

# Process all papers
all_entities = []
all_noun_chunks = []

for paper in papers:
    doc = nlp(paper.content)
    all_entities.extend([ent.text for ent in doc.ents])
    all_noun_chunks.extend([chunk.text for chunk in doc.noun_chunks])

# Find common concepts
entity_freq = Counter(all_entities)
chunk_freq = Counter(all_noun_chunks)
#+END_SRC

**Why**: Fast, reliable, good for exploration

** Phase 2: Deep Analysis

Use **Hugging Face Transformers** + **SBERT**:

#+BEGIN_SRC python
from transformers import pipeline
from sentence_transformers import SentenceTransformer

# Stance detection
stance_classifier = pipeline(
    "text-classification",
    model="kornosk/bert-election2020-twitter-stance-trump"
)

# Semantic similarity
sbert = SentenceTransformer('all-MiniLM-L6-v2')

# Classify stance for each argument
for argument in arguments:
    stance = stance_classifier(argument.text)
    argument.stance = stance

# Find similar arguments across students
embeddings = sbert.encode([arg.text for arg in arguments])
similarities = cosine_similarity(embeddings)
#+END_SRC

**Why**: State-of-the-art accuracy for complex tasks

** Phase 3: Comparative Analysis

Use **vector stores** + **clustering**:

#+BEGIN_SRC python
import chromadb
from sklearn.cluster import AgglomerativeClustering

# Store argument embeddings
client = chromadb.Client()
collection = client.create_collection("student_arguments")

for arg in arguments:
    collection.add(
        documents=[arg.text],
        embeddings=[arg.embedding],
        metadatas=[{"student_id": arg.student_id, "topic": arg.topic}],
        ids=[arg.id]
    )

# Cluster students by argument similarity
student_embeddings = aggregate_by_student(arguments)
clustering = AgglomerativeClustering(n_clusters=5)
student_clusters = clustering.fit_predict(student_embeddings)
#+END_SRC

**Why**: Enables multi-level comparisons and pattern discovery

* Argument Extraction Techniques

** Rule-Based Approach

#+BEGIN_SRC python
def extract_claims_rule_based(doc):
    """
    Use linguistic patterns to identify claims.
    """
    claims = []

    for sent in doc.sents:
        # Look for assertion patterns
        if any(token.text.lower() in ["argue", "claim", "assert", "believe"]
               for token in sent):
            claims.append(sent.text)

        # Look for stance indicators
        if any(token.text.lower() in ["should", "must", "ought"]
               for token in sent):
            claims.append(sent.text)

    return claims
#+END_SRC

** Machine Learning Approach

#+BEGIN_SRC python
from transformers import pipeline

# Use pre-trained argument mining model
argument_classifier = pipeline(
    "text-classification",
    model="chkla/roberta-argument-mining"
)

def extract_claims_ml(doc):
    """
    Use ML model to identify argumentative components.
    """
    claims = []

    for sent in doc.sents:
        result = argument_classifier(sent.text)
        if result[0]['label'] == 'Claim' and result[0]['score'] > 0.7:
            claims.append({
                "text": sent.text,
                "confidence": result[0]['score']
            })

    return claims
#+END_SRC

** Hybrid Approach (Recommended)

#+BEGIN_SRC python
def extract_arguments_hybrid(doc):
    """
    Combine rule-based and ML approaches for robustness.
    """
    # Get candidates from both methods
    rule_claims = extract_claims_rule_based(doc)
    ml_claims = extract_claims_ml(doc)

    # Combine and deduplicate
    all_claims = set(rule_claims + [c['text'] for c in ml_claims])

    # Validate with both approaches
    validated = []
    for claim in all_claims:
        # Must pass both methods or high ML confidence
        if (claim in rule_claims and claim in [c['text'] for c in ml_claims]) or \
           any(c['text'] == claim and c['confidence'] > 0.9 for c in ml_claims):
            validated.append(claim)

    return validated
#+END_SRC

* Stance and Sentiment Analysis

** Stance Detection

Determine whether student is for/against a specific topic:

#+BEGIN_SRC python
def analyze_stance(text, topic):
    """
    Determine stance toward a specific topic.
    """
    # Method 1: Use pre-trained stance detection
    stance_classifier = pipeline("text-classification",
                                 model="stance-detection-model")
    result = stance_classifier(f"{topic}: {text}")

    # Method 2: Sentiment toward topic mentions
    doc = nlp(text)
    topic_sentences = [sent for sent in doc.sents
                      if topic.lower() in sent.text.lower()]

    sentiment_scores = [analyze_sentiment(sent) for sent in topic_sentences]
    avg_sentiment = sum(sentiment_scores) / len(sentiment_scores)

    return {
        "stance": result[0]['label'],  # "favor", "against", "neutral"
        "confidence": result[0]['score'],
        "sentiment": avg_sentiment
    }
#+END_SRC

** Topic-Specific Sentiment

#+BEGIN_SRC python
from transformers import pipeline

sentiment_analyzer = pipeline("sentiment-analysis")

def topic_sentiment_analysis(text, topics):
    """
    Analyze sentiment toward each topic mentioned.
    """
    doc = nlp(text)
    topic_sentiments = {}

    for topic in topics:
        # Find sentences mentioning topic
        relevant_sents = [sent for sent in doc.sents
                         if topic.lower() in sent.text.lower()]

        # Analyze sentiment of those sentences
        if relevant_sents:
            sentiments = [sentiment_analyzer(sent.text)[0]
                         for sent in relevant_sents]
            topic_sentiments[topic] = {
                "avg_score": np.mean([s['score'] for s in sentiments]),
                "label": max(set([s['label'] for s in sentiments]),
                           key=[s['label'] for s in sentiments].count)
            }

    return topic_sentiments
#+END_SRC

* Agreement/Disagreement Detection

** Pairwise Student Comparison

#+BEGIN_SRC python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

sbert = SentenceTransformer('all-MiniLM-L6-v2')

def find_agreement_disagreement(student_papers):
    """
    Create agreement/disagreement matrix across students.
    """
    # Extract main arguments from each paper
    student_arguments = {}
    for paper in student_papers:
        doc = nlp(paper.content)
        arguments = extract_arguments_hybrid(doc)
        student_arguments[paper.student_id] = arguments

    # Encode all arguments
    student_embeddings = {}
    for student_id, args in student_arguments.items():
        # Aggregate argument embeddings
        embeddings = sbert.encode(args)
        student_embeddings[student_id] = np.mean(embeddings, axis=0)

    # Compute pairwise similarity matrix
    students = list(student_embeddings.keys())
    embeddings_matrix = np.array([student_embeddings[s] for s in students])
    similarity_matrix = cosine_similarity(embeddings_matrix)

    return {
        "students": students,
        "similarity_matrix": similarity_matrix,
        "agreement_pairs": find_high_similarity(similarity_matrix, threshold=0.8),
        "disagreement_pairs": find_low_similarity(similarity_matrix, threshold=0.3)
    }
#+END_SRC

** Topic-Specific Agreement

#+BEGIN_SRC python
def topic_agreement_analysis(papers, topic):
    """
    Analyze agreement/disagreement on specific topic.
    """
    stances = {}

    for paper in papers:
        stance = analyze_stance(paper.content, topic)
        stances[paper.student_id] = stance

    # Group by stance
    favor = [sid for sid, s in stances.items() if s['stance'] == 'favor']
    against = [sid for sid, s in stances.items() if s['stance'] == 'against']
    neutral = [sid for sid, s in stances.items() if s['stance'] == 'neutral']

    return {
        "topic": topic,
        "favor": favor,
        "against": against,
        "neutral": neutral,
        "polarization": len(favor) / (len(favor) + len(against)),
        "consensus_level": max(len(favor), len(against)) / len(papers)
    }
#+END_SRC

* Consensus and Dissensus Detection

** Finding Consensus Topics

#+BEGIN_SRC python
def find_consensus_topics(papers, min_agreement=0.7):
    """
    Identify topics where most students agree.
    """
    # Extract all topics mentioned
    all_topics = extract_topics(papers)

    consensus_topics = []

    for topic in all_topics:
        agreement = topic_agreement_analysis(papers, topic)

        if agreement['consensus_level'] >= min_agreement:
            consensus_topics.append({
                "topic": topic,
                "consensus_level": agreement['consensus_level'],
                "dominant_stance": "favor" if len(agreement['favor']) > len(agreement['against']) else "against",
                "students_agreeing": len(agreement['favor']) if len(agreement['favor']) > len(agreement['against']) else len(agreement['against'])
            })

    return sorted(consensus_topics, key=lambda x: x['consensus_level'], reverse=True)
#+END_SRC

** Finding Unique Perspectives

#+BEGIN_SRC python
def find_unique_perspectives(papers, outlier_threshold=0.3):
    """
    Identify students with unique/minority viewpoints.
    """
    # Get argument embeddings for all students
    student_embeddings = get_student_argument_embeddings(papers)

    # For each student, find average similarity to others
    unique_perspectives = []

    for student_id, embedding in student_embeddings.items():
        # Compare to all other students
        other_embeddings = [emb for sid, emb in student_embeddings.items()
                           if sid != student_id]

        similarities = [cosine_similarity([embedding], [other])[0][0]
                       for other in other_embeddings]

        avg_similarity = np.mean(similarities)

        if avg_similarity < outlier_threshold:
            unique_perspectives.append({
                "student_id": student_id,
                "avg_similarity_to_others": avg_similarity,
                "uniqueness_score": 1 - avg_similarity
            })

    return sorted(unique_perspectives, key=lambda x: x['uniqueness_score'], reverse=True)
#+END_SRC

* Analysis Pipeline Example

** Complete Workflow

#+BEGIN_SRC python
class StudentArgumentAnalyzer:
    def __init__(self):
        self.nlp = spacy.load("en_core_web_trf")
        self.sbert = SentenceTransformer('all-MiniLM-L6-v2')
        self.stance_classifier = pipeline("text-classification")

    def analyze_corpus(self, papers, iteration=1):
        """
        Run complete analysis pipeline.
        """
        results = {
            "iteration": iteration,
            "timestamp": datetime.now(),
            "papers_analyzed": len(papers),
            "students": [],
            "topics": [],
            "consensus": [],
            "unique_perspectives": [],
            "agreement_matrix": None
        }

        # Phase 1: Extract arguments from each paper
        for paper in papers:
            doc = self.nlp(paper.content)
            arguments = extract_arguments_hybrid(doc)

            results["students"].append({
                "student_id": paper.student_id,
                "arguments": arguments,
                "embedding": self.sbert.encode(" ".join(arguments))
            })

        # Phase 2: Identify topics
        all_arguments = [arg for student in results["students"]
                        for arg in student["arguments"]]
        results["topics"] = extract_topics(all_arguments)

        # Phase 3: Analyze agreement/disagreement
        results["agreement_matrix"] = find_agreement_disagreement(papers)

        # Phase 4: Find consensus topics
        results["consensus"] = find_consensus_topics(papers)

        # Phase 5: Identify unique perspectives
        results["unique_perspectives"] = find_unique_perspectives(papers)

        return results
#+END_SRC

* Related Topics

- [[file:nlp-tools-comparison.org][NLP Tools and Libraries Comparison]]
- [[file:nlp-mcp-server-integration.org][MCP Server Integration for NLP]]
- [[file:nlp-document-provenance-tracking.org][Document Provenance and Iteration Tracking]]
- [[file:nlp-vector-store-integration.org][Vector Store Integration for Document Comparison]]
- [[file:nlp-hierarchical-embeddings.org][Hierarchical Document Embeddings]]

* TODO Future Enhancements

- TODO Implement evidence type classification
- TODO Add rhetorical strategy detection
- TODO Build student similarity networks visualization
- TODO Create argument pattern templates
- TODO Develop automated report generation
