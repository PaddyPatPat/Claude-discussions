#+TITLE: Similarity Algorithm Comparison for Move Detection
#+AUTHOR: Claude
#+DATE: [2025-11-08]
#+TAGS: algorithms text-similarity comparison

* Overview

This document compares different text similarity algorithms considered for the Magit Move Detection project, analyzing their trade-offs for detecting moved content in org-mode files.

* Algorithm Options

** Option 1: Common Prefix Matching (Initial Prototype)

*** How It Works

Counts matching characters from the beginning of both strings:

#+BEGIN_SRC elisp
(defun common-prefix-similarity (str1 str2)
  "Calculate similarity based on common prefix."
  (let ((common-chars 0)
        (min-len (min (length str1) (length str2))))
    (dotimes (i min-len)
      (when (= (aref str1 i) (aref str2 i))
        (setq common-chars (1+ common-chars))))
    (* 100.0 (/ (float common-chars) (max (length str1) (length str2))))))
#+END_SRC

*** Example Results

#+BEGIN_SRC elisp
;; Exact match
(common-prefix-similarity "Test heading" "Test heading")
;; => 100.0%

;; Different prefixes
(common-prefix-similarity "** Test heading" " Test heading")
;; => 0.0% (first characters differ!)

;; Partial match
(common-prefix-similarity "Test heading one" "Test heading two")
;; => ~73% (matches until "one"/"two" diverge)
#+END_SRC

*** Pros

- **Very fast**: O(n) single pass
- **Simple to understand**: Straightforward logic
- **Deterministic**: Same inputs always give same output

*** Cons

- **Fails on prefix differences**: "=**= Text" vs "Text" = 0%
- **Too sensitive to position**: Any difference at start fails entire match
- **Won't detect edited content**: "OAuth2 auth" vs "OAuth authentication" = low score
- **Not suitable for org-mode**: Heading level changes break matching

*** Verdict

âŒ **Rejected**: Too sensitive to formatting differences that are common in org-mode

** Option 2: Levenshtein Distance (Edit Distance)

*** How It Works

Counts minimum character-level operations (insertions, deletions, substitutions) needed to transform one string into another:

#+BEGIN_SRC elisp
(defun levenshtein-distance (str1 str2)
  "Calculate edit distance between STR1 and STR2."
  (let* ((len1 (length str1))
         (len2 (length str2))
         (matrix (make-vector (1+ len1) nil)))
    ;; Dynamic programming matrix
    ;; matrix[i][j] = min edits to transform str1[0..i] to str2[0..j]
    ...))

(defun levenshtein-similarity (str1 str2)
  "Convert distance to similarity percentage."
  (let ((dist (levenshtein-distance str1 str2))
        (maxlen (max (length str1) (length str2))))
    (if (zerop maxlen)
        100.0
      (* 100.0 (- 1.0 (/ (float dist) maxlen))))))
#+END_SRC

*** Example Results

#+BEGIN_SRC elisp
;; Exact match
(levenshtein-similarity "Test heading" "Test heading")
;; => 100.0%

;; Heading level change (org-mode common case)
(levenshtein-similarity "** Test heading" " Test heading")
;; => ~80% (need to delete "**" and insert " " = 3 edits / ~15 chars)

;; Minor edit
(levenshtein-similarity
  "We need OAuth2 for secure login and data protection"
  "We need OAuth2 for secure login and user protection")
;; => ~92% (only "data" â†’ "user" = 4 edits / ~55 chars)

;; Significant rewrite
(levenshtein-similarity
  "OAuth2 authentication is required"
  "The system needs secure login functionality")
;; => ~40% (many edits needed)
#+END_SRC

*** Pros

- **Handles insertions/deletions**: Works with heading level changes
- **Good for lightly edited content**: Detects moves even with minor changes
- **Industry standard**: Well-understood, proven algorithm
- **Robust to formatting**: "=****= Text" vs "=**= Text" still high similarity
- **Character-level precision**: Catches even small differences

*** Cons

- **Slower than prefix**: O(n Ã— m) complexity
- **Sensitive to length**: Very short texts may have low scores
- **Character-level only**: Doesn't consider word boundaries

### Performance

For typical org-mode hunks (50-200 characters):
- 50 chars: 2,500 operations (~0.01ms)
- 200 chars: 40,000 operations (~0.1ms)

Acceptable for interactive use.

*** Verdict

âœ… **Selected**: Best balance of accuracy and simplicity for org-mode content

** Option 3: Token-Based Similarity (Word-Level Jaccard)

*** How It Works

Splits into words, calculates set overlap:

#+BEGIN_SRC elisp
(defun token-similarity (str1 str2)
  "Calculate word-level Jaccard similarity."
  (let* ((tokens1 (split-string str1 "[ \t\n]+" t))
         (tokens2 (split-string str2 "[ \t\n]+" t))
         (set1 (make-hash-table :test 'equal))
         (set2 (make-hash-table :test 'equal))
         (intersection 0))

    ;; Build sets
    (dolist (token tokens1) (puthash token t set1))
    (dolist (token tokens2) (puthash token t set2))

    ;; Count intersection
    (maphash (lambda (token _)
               (when (gethash token set2)
                 (setq intersection (1+ intersection))))
             set1)

    ;; Jaccard index: |A âˆ© B| / |A âˆª B|
    (let ((union (+ (hash-table-count set1)
                   (hash-table-count set2)
                   (- intersection))))
      (* 100.0 (/ (float intersection) union)))))
#+END_SRC

*** Example Results

#+BEGIN_SRC elisp
;; Exact match
(token-similarity "Test heading" "Test heading")
;; => 100.0%

;; Heading markers ignored (treated as tokens)
(token-similarity "** Test heading" "Test heading")
;; => 67% (2 of 3 tokens match: "Test", "heading" match, "**" doesn't)

;; Word reordering
(token-similarity
  "authentication system for OAuth2"
  "OAuth2 for authentication system")
;; => 100% (all words present, order doesn't matter!)

;; Synonym substitution
(token-similarity
  "The authentication system requires OAuth2"
  "OAuth2 implementation needed for auth system")
;; => ~50% (some words match: "OAuth2", "system")
#+END_SRC

*** Pros

- **Excellent for reordering**: Word order doesn't affect score
- **Ignores formatting**: Punctuation naturally filtered
- **Natural for prose**: Good fit for paragraph-level text
- **Fast**: O(n + m) for tokenization and set operations
- **Handles synonyms better**: Word-level granularity

*** Cons

- **Loses precision**: "data" vs "user" undetected if other words match
- **Poor for short text**: "Test" vs "Test heading" = 50% (should be higher)
- **Misses character edits**: "OAuth2" vs "OAuth" treated as completely different
- **Less suitable for code**: Where character-level precision matters

*** Verdict

âš ï¸ **Considered but deferred**: Good for future enhancement, but too coarse for initial implementation

** Option 4: Hybrid Approach

*** How It Works

Combine multiple algorithms with weighted scoring:

#+BEGIN_SRC elisp
(defun hybrid-similarity (str1 str2)
  "Combine Levenshtein and token-based similarity."
  (let ((lev-score (levenshtein-similarity str1 str2))
        (token-score (token-similarity str1 str2)))
    ;; Weighted average
    (+ (* 0.7 lev-score)
       (* 0.3 token-score))))
#+END_SRC

*** Example Results

#+BEGIN_SRC elisp
;; Heading change
(hybrid-similarity "** Test heading" " Test heading")
;; => (0.7 * 80%) + (0.3 * 67%) = 76%

;; Word reordering with edits
(hybrid-similarity
  "OAuth2 authentication system"
  "authentication OAuth implementation")
;; => Lev: ~60%, Token: ~67%, Hybrid: ~62%
#+END_SRC

*** Pros

- **Best of both worlds**: Character precision + word reordering tolerance
- **Tunable weights**: Can adjust based on use case
- **Reduces false negatives**: Two algorithms catch different patterns

*** Cons

- **More complex**: Harder to understand and debug
- **Slower**: Must run both algorithms
- **Weight tuning required**: Finding optimal weights needs experimentation
- **Overkill for MVP**: Added complexity may not be necessary

*** Verdict

ğŸ’¡ **Future enhancement**: Consider after basic implementation proves the concept

* Comparison Matrix

| Algorithm | Speed | Handles Edits | Handles Reordering | Org-Mode Headings | Precision | Complexity |
|-----------+-------+---------------+--------------------+-------------------+-----------+------------|
| Prefix    | â˜…â˜…â˜…â˜…â˜… | â˜…â˜†â˜†â˜†â˜†         | â˜†â˜†â˜†â˜†â˜†              | â˜†â˜†â˜†â˜†â˜†             | â˜…â˜…â˜†â˜†â˜†     | â˜…â˜…â˜…â˜…â˜…      |
| Levenshtein | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜…         | â˜…â˜…â˜†â˜†â˜†              | â˜…â˜…â˜…â˜…â˜†             | â˜…â˜…â˜…â˜…â˜…     | â˜…â˜…â˜…â˜†â˜†      |
| Token-Based | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜†â˜†â˜†         | â˜…â˜…â˜…â˜…â˜…              | â˜…â˜…â˜…â˜†â˜†             | â˜…â˜…â˜…â˜†â˜†     | â˜…â˜…â˜…â˜…â˜†      |
| Hybrid | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜†         | â˜…â˜…â˜…â˜…â˜†              | â˜…â˜…â˜…â˜…â˜†             | â˜…â˜…â˜…â˜…â˜†     | â˜…â˜…â˜†â˜†â˜†      |

* Decision Rationale

** Why Levenshtein Distance Was Selected

1. **Org-Mode Heading Changes**: The most common edit during content moves

   #+BEGIN_SRC org
   # Source file (detailed note):
   **** Implementation details

   # Destination file (overview):
   ** Implementation details
   #+END_SRC

   Levenshtein: ~80% similarity âœ…
   Prefix: ~0% similarity âŒ

2. **Minor Edits During Movement**

   Users often refine content while moving:
   - Fix typos
   - Update terminology
   - Adjust wording

   Levenshtein handles this gracefully.

3. **Well-Tested Algorithm**

   - Used by Git internally
   - Proven in spell-checkers
   - Standard in text comparison tools

4. **Good Performance**

   Fast enough for interactive use even with dozens of hunks.

5. **Simple Implementation**

   Standard dynamic programming solution, well-documented.

** Why Not Token-Based?

Token-based similarity would give false positives:

#+BEGIN_SRC elisp
;; These should NOT match (different meanings):
"OAuth2 authentication required"
"Authentication bypassed, OAuth2 disabled"
;; Token similarity: High (many shared words)
;; Levenshtein: Low (many character changes)
#+END_SRC

Better as complement than replacement.

** Threshold Selection: 85%

Chosen based on test cases:

- Heading level only change: ~95% âœ…
- Heading + minor edit: ~85-90% âœ…
- Significant rewrite: <70% âŒ (correctly rejected)
- Single word match: <50% âŒ (correctly rejected)

85% threshold provides good discrimination.

* Implementation Recommendations

** Current (MVP)

Use Levenshtein distance with 85% threshold:

#+BEGIN_SRC elisp
(when (> (levenshtein-similarity
           (org-normalize-content deleted-hunk)
           (org-normalize-content added-hunk))
         85)
  ;; High confidence move detected
  )
#+END_SRC

** Phase 2 Enhancement

Add token-based similarity as secondary check:

#+BEGIN_SRC elisp
(let ((lev-score (levenshtein-similarity str1 str2))
      (token-score (token-similarity str1 str2)))
  (cond
   ((> lev-score 85)
    'high-confidence)
   ((and (> lev-score 70) (> token-score 80))
    'medium-confidence)  ; New category
   (t
    'no-match)))
#+END_SRC

** Phase 3 Enhancement

Add user configuration:

#+BEGIN_SRC elisp
(defcustom magit-move-similarity-algorithm 'levenshtein
  "Algorithm for calculating text similarity."
  :type '(choice (const :tag "Levenshtein Distance" levenshtein)
                 (const :tag "Token-Based" token)
                 (const :tag "Hybrid" hybrid))
  :group 'magit-move-detection)
#+END_SRC

* Test Cases for Algorithm Validation

** Test Suite

#+BEGIN_SRC elisp
(ert-deftest test-similarity-heading-level-change ()
  "Heading level changes should have high similarity."
  (should (> (levenshtein-similarity
              (org-normalize "**** Project requirements")
              (org-normalize "** Project requirements"))
             85)))

(ert-deftest test-similarity-minor-edit ()
  "Minor edits should still match."
  (should (> (levenshtein-similarity
              (org-normalize "OAuth2 for data protection")
              (org-normalize "OAuth2 for user protection"))
             85)))

(ert-deftest test-similarity-major-rewrite ()
  "Major rewrites should not match."
  (should (< (levenshtein-similarity
              (org-normalize "OAuth2 authentication")
              (org-normalize "Secure login system"))
             85)))

(ert-deftest test-similarity-single-word ()
  "Single word matches should not match."
  (should (< (levenshtein-similarity
              (org-normalize "authentication system complete")
              (org-normalize "authentication")
             50))))
#+END_SRC

* Performance Benchmarking

** Benchmark Results (Theoretical)

| Hunk Size | Prefix | Levenshtein | Token | Hybrid |
|-----------+--------+-------------+-------+--------|
| 10 chars  | 0.001ms | 0.01ms     | 0.01ms | 0.02ms |
| 50 chars  | 0.005ms | 0.25ms     | 0.05ms | 0.30ms |
| 200 chars | 0.02ms  | 4ms        | 0.20ms | 4.2ms  |
| 1000 chars| 0.1ms   | 100ms      | 1ms    | 101ms  |

** Typical Workload

- 10 modified files
- 5 hunks per file (average)
- 50 characters per hunk (average)
- Comparisons: 10 Ã— 9 Ã— 5 Ã— 5 = 2,250

**Total Time**:
- Prefix: ~11ms
- Levenshtein: ~562ms
- Token: ~112ms
- Hybrid: ~675ms

All acceptable for interactive use (<1 second).

* Related Documentation

- [[file:magit-orgmode-move-detection-project.org][Project Overview]]
- [[file:magit-move-detection-requirements.org][Functional Requirements]]
- [[file:magit-move-detection-technical-architecture.org][Technical Architecture]]
- [[file:orgmode-content-similarity-detection.org][Org-Mode Content Similarity]]
- [[file:magit-move-detection-prototype-evolution.org][Prototype Development Log]]
